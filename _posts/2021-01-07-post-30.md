---
layout: post
title: 'Logistic Regression'
description: '새해 기념 머신러닝 학습하기 #5!'
date: 2021-01-07 16:48:12 +0900
categories: Machine-learning Study
use_math: true
---
## 참고
[망나니개발자님의 블로그][man]

    본 내용은 망나니개발자님이 Coursera에서 Andrew ng 의 Machine Learning(기계학습, 머신러닝)을 수강한 내용을 정리한 것을 바탕으로 작성된 글입니다. 

이번 포스팅에서는 모델과 비용함수(Model and Cost Function)에 대해서 알아보겠습니다.


## 1. Classification

### [ Classification ]
우리가 앞에서 배운 Linear Regression이 주어진 Feature에 대해 Continuous(연속적인)한 Value를 Predict하는 방법이였다면, Classification은 **주어진 Feature에 대해 Data들을 Discrete(이산적인)한 Class로 분류하는 방법**으로 우리가 예측하기를 원하는 값 y는 분류된 클래스들 중 하나의 영역에 속하게 됩니다. 

Classification문제의 예시로는 아래와 같은 것들이 있습니다.

- Email: Spam or Not Spam?

- Online Transactions: Fraudulent(Yes / No)?

- Tumor: Malignant / Benign?

예측값 y가 0이면 Negative Class 그리고 y가 1이면 Positive Class가 됩니다. 물론 Negative 와 Positive의 경계가 애매모호하기도 하고, 실제로는 분류되는 Class가 여러 개로 나누어지는 Multi-class Classification 문제인 경우가 많지만 우리는 0 or 1로 나누어지는 Binary Classification 문제를 먼저 다루어보도록 하겠습니다. 

![그림1](https://t1.daumcdn.net/cfile/tistory/9961FC345A4CAA9F0A)

<br>

### [ Classification for Linear Regression ]

예를 들어 아래의 그림과 같이 Tumor가 Malignant한지 아닌지를 판단하는 Classification 문제가 있다고 할 때, 아직 Classification 문제를 해결하는 방법을 배우지 못한 우리가 할 수 있는 것은 앞에서 배운 Linear Regression을 적용하여 Data에 맞는 직선을 구하는 것입니다. 

아래의 왼쪽 그림과 같은 Training Set에서 $h_θ(x)=θ^Tx$ 를 구하면 파랑색의 직선이 나오고 우리는 예측값이 0.5보다 크면 Malignant라고 예측할 수 있습니다. 그리고 이것은 꽤나 잘 들어맞는 것처럼 보입니다.

- If $h_θ(x)≥0.5$, predict "$y=1$"

- If $h_θ(x)<0.5$, predict "$y=0$"

![그림2](https://t1.daumcdn.net/cfile/tistory/99817D4A5A4CAD392C)

하지만 만약 오른쪽의 그림처럼 새로운 Example이 추가되었다고 할 때, y=0.5 의 기준점인 Tumor Size가 오른쪽으로 치우치면서 직선은 더 눕히게 되고 잘못된 예측을 하게 됩니다. 또한 Linear Regression을 적용하면 $h_θ(x)>1$  or  $h_θ(x)<0$ 인 경우도 발생합니다. 

그래서 우리는 **Classification을 위한 알고리즘으로 $0 \leq h_\theta(x) \leq 1$ 을 만족**하는 Logistic Regression에 대해 배우도록 하겠습니다. 

<small>** 이름은 Logistic Regression이지만 Classfication을 위한 알고리즘이므로 헷갈리시면 안됩니다**</small>

---

## 2. Hypothesis Representation

### [ Hypothesis Representation ]
이번에 알아볼 Hypothesis Representation은 우리의 가설을 표현하기 위한 함수입니다. $ 0 \leq h_\theta(x) \leq 1 $ 을 원하는 상황에서 우리가 위에서 세운 가설함수 $h_θ(x)=θ^Tx$ 는 범위를 만족하지 않으며 잘못된 예측을 할 수 있어서 적합하지 않습니다.

때문에 Logistic Regression 에서는 $h_θ(x)=g(θ^Tx)$ 로 가설함수를 표현합니다. 그리고 여기서 $g(z)=\frac{1}{1+e^{−z}}$ 이므로 이를 정리하여 나타내면 아래와 같습니다. 

$$ h_\theta(x) = \frac{1}{1 + e^{-\theta^Tx}} $$

이 그래프는 0과 1로 수렴하며 그래프의 개형은 아래와 같습니다.

<img src="https://t1.daumcdn.net/cfile/tistory/99F89C335A4CE10F1A" width=500 alt='그림3'>
{: style="text-align:center;"}

덧붙여서 여기에 자연상수 e 대신 다른 수를 집어 넣으면 형태는 비슷하나 기울기가 다른 곡선이 만들어집니다.

<video controls>
    <source src="/assets/imgs/post_30/example.mp4" type="video/mp4">
    Your browser does not support the video tag.
</video> 

이제 우리가 해야 할 것은 파라미터 $θ$ 들을 데이터에 맞추는 것입니다. 우리는 주어진 Training Example로부터 $θ$를 정하고 이 가설함수를 통해 예측을 할 수 있습니다. 

하지만 $θ$ 를 구하는 알고리즘을 배우기 전에 $h_θ(x)$ 에 대해 해석해보도록 하겠습니다.

<br>

### [ Interpretation of Hypothesis Output ]

$$h_θ(x)=P(y=1|x;θ)$$ 

h의 출력값은 Feature X를 가지고 있는 경우에 $y=1$인 Class에 들어갈 확률을 의미합니다. 예를 들어 입력벡터 x가 아래와 같다고 할 때 $h_θ(x) = 0.7$ 이라는 것은 Tumor가 Malignant일 확률이 70%라는 것을 의미합니다. 

또한 결국에 입력 x의 결과는 Class1 또는 Class0에 속해야 하므로 $y=1$ (Class1)일 확률과 $y=2$ (Class2)일 확률의 합은 1이 되어야 합니다. 

$$P(y=0|x;θ)+P(y=1|x;θ)=1$$

![그림4](https://t1.daumcdn.net/cfile/tistory/9902773F5A4D7AFF14)
{: style="text-align:center;"}

---

## 3. Decision Boundary

### [ Logistic Regression ]
이번에 우리가 공부할 내용은 언제 이 가설이 $y=1$임을 예측하고 언제 $y=0$임을 예측하는가(어떠한 경계를 기준으로) 그리고 1개 이상의 Feature를 가질 때 어떤 모습을 보이는가 입니다. 

앞서서 우리는 $h_θ(x)=g(θ^Tx)$ 이며 $g(z)=\frac{1}{1+e^{−z}}$ 임을 배웠고, 아래와 같이 판단한다고 이야기했습니다.

- If $h_θ(x)≥0.5$, predict "$y=1$"

- If $h_θ(x)<0.5$, predict "$y=0$"

위의 그래프의 개형은 아래와 같으며 $h_θ(x)≥0.5$ 인 부분은 $z≥0$ 인 영역과 같습니다.  그렇다면 $y=0$인 Class0에 속하는 경우는 언제일까요? 바로 $z<0$ 인 영역이며 $z=0$을 기준으로 Class가 나뉘어지고 있으므로 Decision Boundary는 $z=0$이 됩니다. 

<img src="https://t1.daumcdn.net/cfile/tistory/99F89C335A4CE10F1A" width=500 alt='그림3'>
{: style="text-align:center;"}

<br>

### [ Decision Boundary ]

$x1$,$x2$ 를 Feature로 갖는 Training Set이 아래와 같이 주어졌다고 하고, 가설함수는 $h_θ(x)=g(θ_0+θ_1x_1+θ_2x_2)$ 라고 할 때, 아직 $θ$ 를 구하는 방법은 배우지 못했으므로 각각의 $θ$ 를 $-3$, $1$, $1$ 이라고 가정하겠습니다. 이때 $h$는 언제 $y=1$ 또는 $y=0$으로 예측을 할까요?

<img src="https://t1.daumcdn.net/cfile/tistory/990D78385A4D863603" width=500 alt='그림5'>
{: style="text-align:center;"}

우리는 위에서 배운 내용들로 $θ^Tx=z$ 에 해당하는 $−3+x_1+x_2≥0$ 이라면 $y=1$로 예측할 것임을 알 수 있고, 이 영역은 위의 초록색 영역으로 나뉘어진 영역들 중 1시 방향의 영역을 의미합니다.

반대로 $z=−3+x_1+x_2<0$인 부분은 7시 방향의 영역으로 $y=0$인 Class에 속하게 되고, 여기서 생긴 경계(직선)이 Decision Boundary가 됩니다. 

정리하자면 **Decision Boundary는 Class들을 나누는 경계**를 의미합니다. 그리고 이러한 Decision Boudary는 Training Set이 아니라 Property로 $θ$ 벡터에 의해 결정이 됩니다.

<br>

### [ Non-Linear Decision Boundaries ]

이번에는 보다 복잡한 예시의 Decision Boundary에 대해 알아보도록 하겠습니다. 

예를 들어 아래의 그림과 같은 Data Set이 주어졌다고 할 때, 직선으로 Class를 구분하는 것은 불가능해보입니다. 가설함수를 $h_θ(x)=g(θ_0+θ_1x_1+θ_2x_2+θ_3x^2_1+θ_4x^2_2)$ 
와 같이 세웠다고 할 때, 5개의 Parameter들이 순차적으로 $-1$, $0$, $0$, $1$, $1$ 을 갖게 된다고 합시다. 그렇다면 우리는 아래와 같이 예측을 할 수 있습니다.

- Predict "$ y=1 $" If $−1+x^2_1+x^2_2≥0$

- Predict "$ y=0 $" If $−1+x^2_1+x^2_2<0$

<img src="https://t1.daumcdn.net/cfile/tistory/992EC4475A4D8A2E35" width=350 alt='그림6'>
<img src="https://t1.daumcdn.net/cfile/tistory/990593425A4D8D770F" width=350 alt='그림7'>
{: style="text-align:center;"}

물론 더 복잡한 경우에는 오른쪽의 그림과 같은 경우도 나올 수 있습니다. 이렇게 실제로는 Non-linear(비선형적인) Decision Boundary를 가지게 되는 경우가 상당히 많은데, 이러한 경우에는 Polynomial(다항식)을 이용하여 Dimension(차수)를 높여 Non-linear Decision Boundary를 표현할 수 있습니다.

---

## 4. Cost Function

### [ Cost Function ]
이번에는 Logistic Regression을 풀기위해 필요한 $θ$ 를 구하는 방법에 대해서 알아보도록 하겠습니다. 우리가 직면한 문제는 아래의 그림과 같고 $θ$ 를 고르는 것이 우리가 해야 할 일입니다.

![페이크](https://lh3.googleusercontent.com/proxy/5MG7CnysGnYopU-XfQllD4jiDh9Bn944ulybYpRMGZJHnuyeKe2KKqTpufy1kA_-TvaALa92N9Li3IZ6XGyBQ9tfmfR5-ER9p3s4zi3YADmn5Mfp-1POZLY06E5Q93Bh26QvpuueMtphwhlr-3hFSFhfTyE8R1ROC8i2J2zdOFTIyUOmwLHL1syB9Mz3_FsfNdww9UFr--G6OK__3tkS3V_mcHVOYA3_OFUnaKPlkxnpa8Ajdb1r4irfqCOfu2ImCVUsk1tsweVgeeZ3cWOttld_I98n4XiYJXT39RpuD0V73fdn1Sjz5TFIKPcTkIKhFOWMvn-oXjxXouQEbYMU6AF1aAYqsAEyfyE)



[man]: https://mangkyu.tistory.com/36