I"%<h2 id="참고">참고</h2>
<p><a href="https://mangkyu.tistory.com/31?category=767742">망나니개발자님의 블로그</a></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>본 내용은 망나니개발자님이 Coursera에서 Andrew ng 의 Machine Learning(기계학습, 머신러닝)을 수강한 내용을 정리한 것을 바탕으로 작성된 글입니다. 
</code></pre></div></div>

<p>이번 포스팅에서는 모델과 비용함수(Model and Cost Function)에 대해서 알아보겠습니다.</p>

<h2 id="1-model모델">1. Model(모델)</h2>
<h3 id="-model-representation-">[ Model Representation ]</h3>
<p>아래의 그림과 같이 다양한 크기의 집과 그 가격이 주어진 그래프가 있을 때, 어떤 한 사람이 1250 feet 크기의 집을 판매하려고 할 때, 얼마에 판매하는 것이 적합한지 우리가 대답을 해주어야 하는 상황입니다.</p>

<p>우리는 데이터들에 맞는 모델을 찾기 위해 직선을 하나 그어 그에 대응하는 가격을 얘기해줄 수 있습니다. 이러한 예시는 우리가 저번에 다루었던 <strong><a href="/posts/post-26">Supervised Learning</a></strong> 중 <strong>Regression</strong>의 예시 중 하나입니다.</p>

<p><img src="https://t1.daumcdn.net/cfile/tistory/99DF553D5A37AFF12A" alt="그림1" /></p>

<p>아래의 그림과 같이 우리에게 주어지는 값들을 <strong>Training Set</strong>이라고 부릅니다.</p>

<p>위의 예시에서는 각 집의 크기와 그에 해당하는 입력값들의 집합을 Training Set이라고 부르고 실제로는 아래의 표와 같이 주어집니다. 우리는 이제 Training Set을 이용하여 1250 feet에 해당하는 집은 얼마의 가격으로 판매하는 것이 적합한지 알아내야 합니다.</p>

<p><img src="https://t1.daumcdn.net/cfile/tistory/99D1FA4B5A37C6BD14" alt="그림2" /></p>

<h3 id="-notation표기법-">[ Notation(표기법) ]</h3>
<ul>
  <li>$m$ : Number of training examples</li>
  <li>$x′s$ : “input” variable or features</li>
  <li>$y′s$ : “output” variable” or “target value”</li>
  <li>$(x,y)$ : one training example</li>
  <li>$(x^i,y^i)$ : i-th training example</li>
  <li>$θ$ : Parameters</li>
</ul>

<p>지도 학습이 동작하는 방식에 대해 설명하기 전에 몇가지 표기법에 대해 정의를 하고 갈 것인데 여기서 $m$은 training 값들의 개수로 위의 그림에서 테이블의 열의 개수에 해당하며, $x$는 입력 변수 또는 특징으로 위의 예시에서는 size에 해당하고, $y$는 출력 변수 또는 타겟 변수로 위의 예시에서는 가격에 해당합니다. 그리고 우리는 단일의 training example을 $(x,y)$로 표기하며 $i$번째 training example을 $(x^i,y^i)$로 표기하겠습니다.</p>

<p><br /></p>

<h3 id="-지도-학습의-작동-">[ 지도 학습의 작동 ]</h3>

<p>이제 주어진 정보들에 대한 설명을 마무리 하였으니 Supervised Learning이 동작하는 Algorithm에 대해서 설명하도록 하겠습니다.</p>

<p>학습을 하는 알고리즘이 있다고 할 때, 주어진 <strong>Training Set을 Learning Algorithm에 넣어 주어야 합니다.</strong> 그러면 Learning Algorithm은 특정한 규칙에 따라서 <strong>새로운 Input과 그에 대해 예측되는 Output을 지니는 함수 h(hypothesis)를 제시해줍니다.</strong> 그리고 우리는 우리가 원하는 x에 대응하는 값 y를 구하기 위해 h함수에 x를 대입하고 이때 얻은 값이 우리가 원하는 값이 됩니다.</p>

<p><img src="https://t1.daumcdn.net/cfile/tistory/9914803C5A37B90828" alt="그림3" /></p>

<p><br /></p>

<h3 id="-가설-함수hypothesis-function-">[ 가설 함수(Hypothesis Function) ]</h3>

<p>그 다음으로 우리는 가설 함수 h를 어떻게 표현할 것인지에 대해서 정의해야 하는데, 우리는 여기서 예측기가 되는 h의 함수를 $h_θ(x)=θ_0+θ_1x$로 정의할 수 있고, 이 식은 종종 $h_θ(x)$ 에서 $θ$를 제거하여 $h(x)$로 표현하곤 합니다.</p>

<p>그리고 이 함수가 의미하는 것은 $y$가 아래와 같은 선형함수라고 예측을 하는 것입니다. <strong>우리는 이를 단순 선형 회귀, Linear regression with one variable 또는 Univariate linear regression이라고 합니다.</strong> 그래서 위의 예시에서 주어진 Training Set을 기반으로 직선인 함수를 구하게 되고 1250 feet를 입력하게 됩니다. 물론 실제로는 비선형 함수를 사용하곤 하지만 선형 유형이 가장 간단한 빌딩 블록이므로 선형 함수에서 시작하여 비선형의 함수로 도달하도록 하겠습니다.</p>

<p><img src="https://t1.daumcdn.net/cfile/tistory/9912F34F5A37BD8234" alt="그림4" /></p>

<hr />

<h2 id="2-cost-function비용함수">2. Cost Function(비용함수)</h2>
<p>Cost Function은 입력한 Training Set에 대하여 가장 적합한 직선을 우리가 가질 수 있게 해줍니다. 우리는 다음과 같은 가설함수  $h_θ=θ_0+θ_1x$ 에서 매개변수(Parameter)에 해당하는 $θ_0,θ_1$을 결정해주어야 합니다. $θ$의 값들에 따라 함수의 개형이나 값이 아래의 그림처럼  달라지기 때문에 가장 $y$에 근사하도록 Training Set을 통해서 정해주어야합니다.</p>

<p><img src="https://t1.daumcdn.net/cfile/tistory/994EBA385A38D1662C" alt="그림5" /></p>

<p>우리는 $h_θ(x)−y$의 값이 최소가 되기를 원하지만 실제의 Error(오차)는 양수 또는 음수이므로 <strong>$(h_θ(x)−y)^2$ 이 최소가 되는 것을 구하는 것</strong>이 합리적입니다.</p>

<p>실제로 Training Set는 $1$부터 $m$ 까지 존재하기 때문에 $1$ ~ $m$에 존재하는 모든 차이를 합해서 개수만큼 나누어 평균이 최소가 되는 $θ$를 구해야합니다. 그리고 이를 공식으로 나타내면 아래와 같습니다.</p>

\[J(θ_0,θ_1) = \frac{1}{2m}\sum_{i=1}^m(h_θ(x^i)−y^i)^2\]

<p style="text-align: center;"><small>(2를 나누어 주는 것은 뒤에서 미분을 할 때계산의 편리함을 위한 것입니다.)</small></p>

<p>즉, 우리가 구한 $J(θ_0,θ_1)$ 가 Cost Function(비용함수)가 되고 이 함수는 이미지 또는 동영상 등과 같은 영역에서 자주 사용되는 Mean Square Error(MSE)와 같습니다. 물론 이 비용함수는 MSE외에도 SNR, PSNR 등이 있지만 MSE가 가장 보편적으로 사용되는 함수입니다.</p>

<p>즉 정리하자면 <strong>비용함수는 원래의 값과 오차가 가장 적은 $θ$를 구하여 가설함수 $h$를 정하는데 사용되는 함수</strong>와 같습니다.</p>

<p><br /></p>

<h3 id="-중간-요약-">[ 중간 요약 ]</h3>
<ul>
  <li>Hypothesis: $h_θ(x)=θ_0+θ_1x$</li>
  <li>Parameters: $θ_0,θ_1$</li>
  <li>Cost Function: $J(θ_0,θ_1)=\frac{1}{2m}\sum_{i=1}^m(h_θ(x^i)−y^i)^2$</li>
  <li>Goal: minimize $θ_0,θ_1 -&gt; J(θ_0,θ_1)$</li>
</ul>

<p><br /></p>

<h3 id="-intuition-ⅰ">[ Intuition Ⅰ]</h3>

<p>$θ$에 대한 Cost Function을 직관적으로 이해하기 위해서 가설함수 $h$를 $θ_0=0$인 경우로 단순화시킨 $h_θ(x)=θ_1x$ 의 비용함수 $J(θ_1)$에 대해 먼저 알아보겠습니다.</p>

<p>이 가설함수는 $(0,0)$을 지나며 $J(θ_1)=\frac{1}{2m}\sum_{i=1}^m(θ_1x^i−y^i)^2$와 같은 비용함수를 갖게 됩니다. $(1,1),(2,2),(3,3)$의 Training Set를 갖는 경우에 대하여 3가지 θ에 대한 비용은 아래와 같습니다.</p>

<table>
  <tbody>
    <tr>
      <td>$If)~θ_1=1$</td>
      <td>$h_θ(x)=x$</td>
      <td>$J(θ_1)=0$</td>
    </tr>
    <tr>
      <td>$If)~θ_1=0.5$</td>
      <td>$h_θ(x)=0.5x$</td>
      <td>$J(θ_1)≈0.58$</td>
    </tr>
    <tr>
      <td>$If)~θ_1=0$</td>
      <td>$h_θ(x)=0$</td>
      <td>$J(θ_1)≈2.3$</td>
    </tr>
  </tbody>
</table>

<p>추가적으로 수많은 Training Set를 가지고 점들을 그려보면 아래와 같은 함수가 나오게 됩니다.</p>

<p>아래의 함수에서 우리에게 필요한 것은 $J(θ_1)$의 값 즉, $y$ 의 값이 최소가 되는 경우이므로 $θ_1=1$인 경우의 $θ$가 우리가 찾는 값이 됩니다. Cost Function은 근본적으로 제곱의 합을 평균낸 것이므로 <strong>$J(θ)=0$ 인 경우가 가장 이상적인 경우</strong>입니다.</p>

<p style="text-align: center;"><img src="https://t1.daumcdn.net/cfile/tistory/996CDC365A39BCFC0E" alt="그림6" /></p>

<p><br /></p>

<h3 id="-intuition-ⅱ-">[ Intuition Ⅱ ]</h3>

<p>이번에는 위의 경우보다 복잡한 $θ_0,θ_1$이 존재하는 경우를 원래의 예시인 집의 크기와 가격이 주어진 상황으로 알아보도록 하겠습니다.</p>

<p>예를 들어 $θ_0=50,  θ_1=0.06$ 인 경우는 가설함수 h가 왼쪽의 그림과 같이 그려지며 $J(θ_0,θ_1)$ 의 값은 왼쪽의 초록색 선들을 통해 구한것 처럼 가설함수와 모든 Data Set에 대한 길이 차이의 제곱을 합한 평균이 됩니다.</p>

<p>그리고 이러한 Cost Function을 일반화시켜 표현하면 $θ_0,θ_1,J(θ_0,θ_1)$ 총 3가지를 표현해야하므로 오른쪽과 같은 3차원의 그림이 됩니다. 그리고 우리에게 필요한 J(θ0,θ1) 의 값은 오른쪽 그림의 빨간선처럼 해당 값으로부터의 높이 즉, (X,Y) 평면으로부터의 높이가 됩니다. 그리고 앞으로는 오른쪽 그림과 같은 3차원의 도형이 아닌 3차원의 그림을 (X,Y) 평면으로 정사영시킨 등고선 그래프(Contour Plots, Contour Figures)를 활용하도록 하겠습니다.</p>

:ET