I"<h2 id="참고">참고</h2>
<p>[망나니개발자님의 블로그][man]</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>본 내용은 망나니개발자님이 Coursera에서 Andrew ng 의 Machine Learning(기계학습, 머신러닝)을 수강한 내용을 정리한 것을 바탕으로 작성된 글입니다. 
</code></pre></div></div>

<p>이번 포스팅에서는 모델과 비용함수(Model and Cost Function)에 대해서 알아보겠습니다.</p>

<h2 id="1-gradient-descent경사하강법">1. Gradient Descent(경사하강법)</h2>

<h3 id="-gradient-descent-">[ Gradient Descent ]</h3>

<p>Gradient Descent Algorithm은 <strong>Cost Function $J(θ_0,θ_1)$ 을 최소로 만드는 $θ_0,θ_1$ 을 구하는 알고리즘</strong>으로 Linear Regression뿐만 아니라 머신러닝(기계학습)에서 전반적으로 사용되는 알고리즘입니다.</p>

<p>Gradient Descent는 먼저 $θ_0,θ_1$ 에 대한 <em>임의의</em> 초기값으로 시작합니다. 그리고 최소의 $J(θ_0,θ_1)$ 을 찾을 때 까지 $θ_0,θ_1$ 을 변경시킵니다. 즉, 이 알고리즘은 임의의 초기값을 기준으로 최소가 되는 점을 찾아나갑니다.</p>

<p><img src="https://t1.daumcdn.net/cfile/tistory/99A123355A3A2DAF12" alt="그림1" /></p>

<p>Gradient Descent Algorithm을 수학적으로 정의하면 다음과 같습니다.</p>

<p>repeat until convergence{</p>

\[θ_j:=θ_j−α\frac{∂}{∂θ_j}J(θ_0,θ_1)\]

<p>}</p>

<ul>
  <li>$:=$ : Assignment Operator (대입 연산자)</li>
  <li>$α$ : Learning Rate (학습 속도)</li>
  <li>$\frac{∂}{∂θ_j}J(θ_0,θ_1)$ : Derivative Term (미분 계수)</li>
</ul>

<p>여기서 <strong>$:=$</strong> 는 <em>대입 연산자</em>로 <strong>$θ_j$ 에 $θ_j−α□$ 을 대입하겠다는 의미</strong>입니다.</p>

<p><strong>$α$</strong> 는 <em>Learning Rate</em>로 위의 그림에서처럼 <strong>언덕에서 내려갈 때 얼마나 큰 걸음으로 걸을 것인가를 나타내는 양의 상수</strong>이므로 $α$ 가 클수록 한 Step에 움직이는 길이가 길어집니다.</p>

<p><strong>$\frac{∂}{∂θ_j}J(θ_0,θ_1)$</strong> 는 <strong>비용함수를 $θ$에 대해 미분한 함수</strong>입니다. 미분해주는 값은 $J(θ_0,θ_1)$ 즉, 비용함수로 $θ_0,θ_1$ 총 2가지 변수에 의해 영향을 받고 있습니다.</p>

<p>우리는 이러한 대입의 과정을 지속해주어야 하는데, $θ_1$의 값을 대입할 때 비용함수가 영향을 받으므로 왼쪽의 그림 처럼 Simultaneous 하게 동시에 Update를 해주어야 합니다. 지금부터 기울기 하강 알고리즘을 구성하는 요소들에서 Learning Rate α 가 무엇을 하는지 그리고 왜 미분계수와 α 를 곱한 α∂∂θjJ(θ0,θ1)를 이용하는지 이해를 돕기위해 하나의 Parameter θ1 만을 갖는 경우에 대해서 알아보도록 하겠습니다.</p>
:ET