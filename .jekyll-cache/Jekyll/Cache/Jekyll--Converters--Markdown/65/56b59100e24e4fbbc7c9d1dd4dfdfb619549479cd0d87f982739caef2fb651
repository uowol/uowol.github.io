I"Y<h2 id="참고">참고</h2>
<p>[망나니개발자님의 블로그][man]</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>본 내용은 망나니개발자님이 Coursera에서 Andrew ng 의 Machine Learning(기계학습, 머신러닝)을 수강한 내용을 정리한 것을 바탕으로 작성된 글입니다. 
</code></pre></div></div>

<p>이번 포스팅에서는 Overfitting(과적합)문제와 이를 해결하기 위한 Regularization(정규화)에 대해서 알아보도록 하겠습니다.</p>

<h2 id="1-the-problem-of-overfitting">1. The Problem of overfitting</h2>

<h3 id="-example-linear-regression-">[ Example: Linear Regression ]</h3>

<p><img src="https://t1.daumcdn.net/cfile/tistory/99EAD14A5A53024632" alt="그림1" /></p>

<p>위와 같은 Linear Regression 집값 예측 문제가 있다고 할 때, 직선의 가설함수를 세울 때, 이차함수의 가설함수를 세울 때, 다차 함수의 가설함수를 세울 때 각각의 상황에 대한 판단이 아래와 같이 내려집니다.</p>

<ul>
  <li>
    <p>직선: Underfitting or high-bias</p>
  </li>
  <li>
    <p>이차함수: Just Right</p>
  </li>
  <li>
    <p>다차함수: Overfitting</p>
  </li>
</ul>

<p>직선으로 가설함수를 세우는 경우에는 데이터가 가설함수에 잘 들어맞지 않고 강한 선입견(Pre-Conceptual)을 갖는다고 하여 Underfitting 또는 high-bias 문제를 갖고 있다고 합니다.</p>

<p>2차함수의 가설함수는 size가 커지면 감소하기 때문에 적합한 가설함수는 아니지만 그래도 주어진 데이터들은 꽤나 잘 들어맞는 것으로 보입니다.</p>

<p>다차함수의 가설함수는 지나치게 데이터를 맞추려고 하다보니 커브가 많이 생기고 휘어지는 등 High Variance(높은 분산) 또는 Overfitting(과적합) 문제가 발생합니다. 물론 주어진 훈련용 데이터에 관해서는 매우 좋은 성능을 나타내지만 새로운 Data가 왔을 때 제대로 된 예측을 하기에는 부족합니다.</p>

<p>즉,<strong>많은 Feature를 가지고 있는 경우에 가설함수가 훈련용 데이터에 관해서는 비용함수가 0에 근접할 정도로 매우 잘 맞지만 새로운 데이터들이 오면 일반화를 하는데 실패하는 것</strong>을 과적합 문제라고합니다.</p>

<p>어떤 고차함수를 데이터에 맞추려고 할 때 맞출수는 있지만 <strong>커다란 변동성</strong>을 갖게 되는데, 이러한 변동성에 의해 새로운 examples에 대해서 제대로된 예측을 하지 못하는 문제입니다.</p>

<p><br /></p>

<h3 id="-example-logistic-regression-">[ Example: Logistic Regression ]</h3>

<p><img src="https://t1.daumcdn.net/cfile/tistory/997798345A530D8D19" alt="그림2" /></p>

<p>예를 들어 위와 같은 Logistic Regression 문제가 있다고 할 때, 3가지 가설함수 역시도 각각 다른 모습을 보여줍니다.</p>

<ul>
  <li>
    <p>직선: Underfitting or High bias</p>
  </li>
  <li>
    <p>이차식: Just Right</p>
  </li>
  <li>
    <p>다차식: Overfitting or High variance</p>
  </li>
</ul>

<p>첫번째 직선의 경우는 직선과 데이터들이 잘 들어맞지 않습니다.</p>

<p>두번째 포물선의 경우에는 우리가 얻을 수 있는 최적의 선으로 보입니다.</p>

<p>마지막으로 엄청 고차원의 가설함수를 만들면 데이터에 딱 맞는 자잘자잘 꼬여진 Decision Boundary를 갖게 되지만 제대로 된 예측을 해내지 못합니다. 이를 Overfitting or High Variance 문제라고 합니다.</p>

<p><br /></p>

<h3 id="-addressing-overfitting-">[ Addressing overfitting ]</h3>

<p>그렇다면 먼저 과적합인지를 어떻게 알아낼 수 있을까요?</p>

<ul>
  <li>
    <p>가설함수의 그래프를 그려보기</p>
  </li>
  <li>
    <p>학습데이터가 너무 적지 않은지 검사하기</p>
  </li>
</ul>

<p>그리고 이러한 과적합 문제를 해결하기 위해서는 아래의 2가지 방법을 수행하면 됩니다.</p>

<ul>
  <li>
    <p>Reduce number of features</p>
  </li>
  <li>
    <p>Regularization</p>
  </li>
  <li>
    <p><del>Increase number of training data</del></p>
    <ul>
      <li><small>학습 데이터를 늘리면 과적합 문제를 해결할 수 있지만 일반적으로 어려운 일입니다.</small></li>
    </ul>
  </li>
</ul>

<p>먼저 Feature들 중에서도 중복이 되거나 예측을 하는데 중요한 요소가 아닌 Feature들이 있을 수 있습니다. 이럴 땐 <strong>불필요한 Feature들을 먼저 제거</strong>해주거나 알고리즘이 어떤 Feature을 사용할 것인지 자동으로 선택해주는 <em>Model Selection Algorithm</em> 을 이용할 수 있습니다.</p>

<p>그리고나서 Regularization(정규화)를 해주면 되는데, 정규화는 <strong>모든 Feature들을 유지하되 $θ$의 값을 줄이는 것</strong>으로 $y$를 예측하기 위한 수많은 Feature들이 존재하며 그 Feature들이 y에 영향을 주는 경우에 잘 작동합니다. 즉, Regularization은 모델이 너무 복잡해지지 않도록 임의로 제약을 가하는 것입니다.</p>

<hr />

<h2 id="2-cost-function">2. Cost Function</h2>

<h3 id="-intuition-">[ intuition ]</h3>

<p><img src="https://t1.daumcdn.net/cfile/tistory/990DAE435A5314AC12" alt="그림3" /></p>

<p>위와 같은 Linear Regression 집값 예측 문제에서 왼쪽의 경우는 잘들어맞는 경우이며 오른쪽의 경우는 Overfitting이 걸린 경우입니다.</p>

<p>그래서 우리는 $θ_3$와 $θ_4$를 최소화하여 $x^3$과 $x^4$의 영향력을 줄이려 합니다. 그렇게 되어 $θ_3$와 $θ_4$가 $0$에 근접한다면 가설함수는 2차함수에 근접해는 동시에 더욱 간단하며 smooth해질 뿐 아니라 결과적으로 Overfitting하는 경향이 줄어들 것입니다.</p>

<p>그 방법으로 $θ_3$와 $θ_4$에 각각 큰 패널티(Penalty)를 부여하면(여기서는 1000) 학습 알고리즘은 학습 오류를 최소화하는 것보다 $\theta$를 최소화하는데 우선 순위를 둘 것입니다.</p>

<p stlye="text-align:center;"><img src="https://t1.daumcdn.net/cfile/tistory/9956CE3F5A3A6F4235" width="400" alt="그림4" />
<br />
<small>🔺이전에 공부했던 학습 알고리즘</small></p>

<p>$θ$들을 구한 후에는 다시 Penalty를 제거함으로 $θ_3$와 $θ_4$를 최소화시키자는 아이디어입니다. 이렇게 하면 4차함수 모델을 2차함수의 모델에 거의 근접하도록 만들 수 있습니다.</p>

\[\min_θ\frac{1}{2m}\sum_{i=1}^m(h_θ(x^{(i)})−y^{(i)})^2\color{red}{+1000}θ^2_3\color{red}{+1000}θ^2_4\]
:ET